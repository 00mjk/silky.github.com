<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>silky.github.io</title>
    <link href="https://silky.github.io/atom.xml" rel="self" />
    <link href="https://silky.github.io" />
    <id>https://silky.github.io/atom.xml</id>
    <author>
        <name>Noon van der Silk</name>
        <email>noonsilk+-noonsilk@gmail.com</email>
    </author>
    <updated>2016-12-11T00:00:00Z</updated>
    <entry>
    <title>Quantum neural networks</title>
    <link href="https://silky.github.io/posts/2016-12-11-quantum-neural-networks.html" />
    <id>https://silky.github.io/posts/2016-12-11-quantum-neural-networks.html</id>
    <published>2016-12-11T00:00:00Z</published>
    <updated>2016-12-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="info">
    Posted on December 11, 2016
    
        by Noon van der Silk
    
</div>

<p>(This post requires a background in the basics of quantum computing (and neural networks). Please have a read of the first part of <a href="/posts/2014-09-09-intro-to-qc-and-the-surface-code.html">Introduction to quantum computing and the surface code</a> if you’d like to get up to speed on the quantum parts, <a href="http://neuralnetworksanddeeplearning.com/">Neural networks and Deep Learning</a> is a good introduction to the other part.)</p>
<p>Recently, I’ve been spending a lot of time thinking about machine learning, and in particular deep learning. But before that, I was mostly concerning myself with quantum computing, and specifically the algorithmic/theory side of quantum computing.</p>
<p>In the last few days there’s been a flurry of papers on quantum machine learning/quantum neural networks, and related topics. Infact, there’s been a fair bit of research in the last few years (see the <a href="#appendix">Appendix</a> at the end for a few links), and I thought I’d take this opportunity to have a look at what people are up to.</p>
<p>The papers we’ll be discussing are:</p>
<ul>
<li><a href="https://scirate.com/arxiv/1612.01045">arXiv:1612.01045 - Quantum generalisation of feedforward neural networks</a> by Wan, Dahlsten, Kristjánsson, Gardner and Kim.</li>
<li><a href="https://scirate.com/arxiv/1612.01789">arXiv:1612.01789 - Quantum gradient descent and Newton’s method for constrained polynomial optimization</a> by Rebentrost, Schuld, Petruccione and Lloyd,</li>
<li><a href="https://scirate.com/arxiv/1612.02806">arXiv:1612.02806 - Quantum autoencoders for efficient compression of quantum data</a> by Romero, Olson and Aspuru-Guzik,</li>
</ul>
<p>But first, let’s take a look at the paper that got me interested in machine learning in the first place!</p>
<h2 id="arxiv1307.0411---quantum-algorithms-for-supervised-and-unsupervised-machine-learning">arXiv:1307.0411 - Quantum algorithms for supervised and unsupervised machine learning</h2>
<p>The paper, <a href="https://scirate.com/arxiv/1307.0411">Quantum algorithms for supervised and unsupervised machine learning</a> by Lloyd, Mohseni and Rebentrost in 2013, was one of my first technical exposures to machine learning. It’s an interesting one because it demonstrates that for certain types of clustering algorithms there is a quantum algorithm that exhibits an exponential speed-up over the classical counterpart.</p>
<p><strong>Aside</strong>: Gaining complexity-theoretic speed-ups is the central task of (quantum) complexity theory. The speedup in this paper is interested, but it “only” demonstrates a speed-up on a problem that is already known to be <em>efficient</em><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> for classical computers, so it <em>doesn’t</em> provide evidence that quantum computers are fundamentally more powerful than classical ones, by the standard notions in computer science.</p>
<p>The <code>Supervised clustering</code> problem that is tackled in the paper is as follows:</p>
<blockquote>
<p>Given some vector <span class="math inline">\(\vec{u} \in \mathbb{R}^N\)</span>, and two sets of vectors <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span>, then given <span class="math inline">\(M\)</span> representative samples from <span class="math inline">\(V\)</span>: <span class="math inline">\(\vec{v}_j \in V\)</span> and <span class="math inline">\(\vec{w}_k \in W\)</span>, figure out which set <span class="math inline">\(\vec{u}\)</span> should go in to, by comparing the distances to these vectors.</p>
</blockquote>
<p>In pictures it looks like so:</p>
<div style="text-align: center">
<div class="figure">
<img src="../images/supervised-clustering.png" alt="Figure 1. An example of the supervised clustering problem with \vec{u} \in \mathbb{R}^2 and M=3. We’ve drawn 3 points from V, the (unknown) dashed purple region, and 3 points from W, the dashed pink region." />
<p class="caption">Figure 1. An example of the supervised clustering problem with <span class="math inline">\(\vec{u} \in \mathbb{R}^2\)</span> and <span class="math inline">\(M=3\)</span>. We’ve drawn 3 points from <span class="math inline">\(V\)</span>, the (unknown) dashed purple region, and 3 points from <span class="math inline">\(W\)</span>, the dashed pink region.</p>
</div>
</div>
<p>Classically, if we think about where we’d like to put <span class="math inline">\(\vec{u}\)</span>, we could compare the distance to all the points <span class="math inline">\(\vec{v_1}, \vec{v_2}, \vec{v_3}\)</span> and to all the points <span class="math inline">\(\vec{w_1}, \vec{w_2},\vec{w_3}\)</span>. In the specific example I’ve drawn, doing so will show that, out of the two sets, <span class="math inline">\(\vec{u}\)</span> belongs in <span class="math inline">\(V\)</span>.</p>
<p>In general, we can see that, using this approach, we would need to look at all <span class="math inline">\(M\)</span> data points, and we’d need to compare each dimension of the <span class="math inline">\(N\)</span> dimensions of each vector <span class="math inline">\(\vec{v_j}, \vec{w_k}, \vec{u}\)</span>; i.e. we’d need to look at at least <span class="math inline">\(M\times N\)</span> pieces of information. In other words, we’d compute the distance</p>
\begin{align*}
    d(\vec{u}, V) = \left| \vec{u} - \frac{1}{M}\sum_{j=1}^{M} \vec{v_j} \right|
\end{align*}
<p>By looking at the problem slightly more formally, we find that classically the best known algorithm takes “something like”<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> <span class="math inline">\(\texttt{pol}y(M\times N)\)</span> steps, where “<span class="math inline">\(\texttt{poly}\)</span>” means that the true running time is a polynomial of <span class="math inline">\(M\times N\)</span>.</p>
<p>Quantumly, the paper demonstrates an algorithm that lets us solve this problem in “something like” <span class="math inline">\(\log(M\times N)\)</span> time steps. This is a significant improvement, in a practical sense. To get an idea, if we took <span class="math inline">\(100\)</span> samples from a <span class="math inline">\(N = 350\)</span>-dimensional space, then <span class="math inline">\(M\times N = 35,000\)</span> and <span class="math inline">\(\log(M \times N) \approx 10\)</span>.</p>
<p>The quantum algorithm works by constructing a state in a certain state so that, when measured, the distance that we wanted, <span class="math inline">\(d(\vec{u}, V)\)</span>, is the probability that we achieve a certain measurement outcome. In this way, we can build this certain state, and measure it, several times, and use this information to approximate the required distances. And, the paper shows that this whole process can be done in “something like” a running time of <span class="math inline">\(\log(M\times N)\)</span>.</p>
<p>There’s more contirbutions in the paper than just this; so it’s worth a look if you’re interested.</p>
<h2 id="arxiv16.12.01045---quantum-generalisation-of-feedforward-neural-networks">arXiv:16.12.01045 - Quantum generalisation of feedforward neural networks</h2>
<p>So this paper is pretty cool. We can get a feel for what it’s doing by first considering the following network:</p>
<div style="text-align: center">
<div class="figure">
<img src="../images/basic-nn.png" alt="Figure 2. A simple classical neural network" />
<p class="caption">Figure 2. A simple classical neural network</p>
</div>
</div>
<p>This network has two inputs, <span class="math inline">\(x_1, x_2\)</span>, three learnable weights, <span class="math inline">\(w_1, w_2, w_3\)</span>, one output value <span class="math inline">\(y\)</span>, and an activation function <span class="math inline">\(f\)</span>.</p>
<p>Classically one would feed in a series of training examples <span class="math inline">\((x_1, x_2, y)\)</span> and update the weights according to some loss function to achieve the best result for the given data.</p>
<p>Quantumly, there are some immediate problems with doing this, if we switch the inputs <span class="math inline">\(x\)</span> to be quantum states, instead of classical real variables.</p>
<p>The problems are:</p>
<ul>
<li>Quantum operations need to be reversible; <span class="math inline">\(f\)</span> as written is not (at the very least, it takes two inputs and squashes them down to one output).</li>
<li>Quantum states need to be normalised; so multiplying them by a single arbitrary weight won’t be productive.</li>
<li>You can’t copy arbitrary quantum states due to the <a href="https://en.wikipedia.org/wiki/No-cloning_theorem">No-Cloning theorem</a>, so this restricts the type of networks that can be written down.</li>
</ul>
<p>The way this paper solves these problems is to transition Figure 2 from a classical non-reversible network to a reversible quantum one:</p>
<div style="text-align: center">
<div class="figure">
<img src="../images/transition-to-quantum-nn.png" alt="Figure 3. The transition from classical, to reversible classical, to quantum." />
<p class="caption">Figure 3. The transition from classical, to reversible classical, to quantum.</p>
</div>
</div>
<p>The final network takes in an arbitrary quantum state of two qubits, <span class="math inline">\(x_1, x_2\)</span>, and then adjoins an ancilla state <span class="math inline">\(|0\rangle\)</span>, applies some unitary operation <span class="math inline">\(U\)</span>, and emits a combined final state <span class="math inline">\(|\psi\rangle^{\text{Out}}_{x_1,x_2,y}\)</span> where the final qubit <span class="math inline">\(y\)</span> contains the result we’re interested in.</p>
<p>At this point, one might reasonably ask: How is this different to a quantum circuit? It appears to me that the only difference is that <span class="math inline">\(U\)</span> is actually unknown, and <em>it</em> is trainable! Note that this is also a somewhat radical difference from classical neural networks: there, we don’t normally think of the activation functions (defined as <span class="math inline">\(f\)</span> above) as trainable parameters; but quantumly, in this paper, that’s exactly how we think of them!</p>
<p>It turns out that unitary matrices can be parameterised by a collection of real variables <span class="math inline">\(\alpha\)</span>. Consider an arbitrary unitary matrix operating on two qubits, then <span class="math inline">\(U\)</span> can be written as:</p>
\begin{align*}
    U = \exp\left[ i \left(
             \sum_{j_1,j_2=0,0}^{3,3} \alpha_{j_1, j_2} \times \left(\sigma_{j_1}
             \otimes \sigma_{j_2}\right) \right) \right]
\end{align*}
<p>where <span class="math inline">\(\sigma_i, i \in {1,2,3}\)</span> are the usual <a href="https://en.wikipedia.org/wiki/Pauli_matrices">Pauli matrices</a> and <span class="math inline">\(\sigma_0\)</span> is the <span class="math inline">\(2\times 2\)</span> identity matrix. So one can then make these parameters <span class="math inline">\(\alpha_{j_1, j_2}\)</span> the <em>trainable</em> parameters! <s>It turns out that in the paper they don’t train these parameters explicitly, instead they pick a less general way of writing down unitary matricies, and they construct, by hand, a unitary for two qubits. It’s not clear why they’ve done this, and it would not be fun to have to build a special trainable unitary matrix for each node/neuron of your architecture depending on its input.</s></p>
<p><strong>Update:</strong> Kwok-Ho kindly corrected me that they <em>do</em> indeed train directly on this form of unitary matricies, and that the simplification they do in the paper is used to investigate the loss surface.</p>
<p>In any case, the main contribution of this paper seems to me to be the idea that we can <em>learn</em> unitary matricies for our particular problem. They go on to demonstrate that this idea works to build a quantum autoencoder, and to make a neural network discover unitary matricies that perform the quantum teleportation protocol.</p>
<p>One view is that trying to learn arbitrary unitary matrices that perform a task really well will become too hard as the number neurons grows. If we had a large network, with potentially millions of internal, neurons (and hence unitaries) to learn, then it might be more effective to fix unitaries and instead focus on learning the weights.</p>
<p>However, it’s a promising technique that would be fun to try out.</p>
<h2 id="arxiv1612.01789---quantum-gradient-descent-and-newtons-method-for-constrained-polynomial-optimization">arXiv:1612.01789 - Quantum gradient descent and Newton’s method for constrained polynomial optimization</h2>
<p>Those of you familiar with neural networks will know that the central idea used to train them is <a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a>. We recall that gradient descent lets us known how to modify some vector <span class="math inline">\(x\)</span> so that it does “better” when evaluated with some cost function <span class="math inline">\(C(x, y)\)</span> where <span class="math inline">\(y\)</span> is some known good answer. I.e. <span class="math inline">\(x\)</span> might be a probability of liking some object, and <span class="math inline">\(y\)</span> might be the true probability, and <span class="math inline">\(C(x,y) = |x-y|^2\)</span>.</p>
<p>The paper supposes we have some quantum state <span class="math inline">\(|x\rangle = \sum_{j=1}^N x_j |j\rangle\)</span> (where <span class="math inline">\(|j\rangle\)</span> is the <span class="math inline">\(j\)</span>’th computational-basis state), and some cost function <span class="math inline">\(C(|x\rangle, |y\rangle)\)</span> that tells us how good <span class="math inline">\(|x\rangle\)</span> is. The question is, given we can evaluate <span class="math inline">\(C(|x\rangle, |y\rangle)\)</span>, how can we best work out to modify <span class="math inline">\(|x\rangle\)</span> to do better?</p>
<p>If this was entirely classical, we could just calculate the gradient of <span class="math inline">\(C\)</span> with respect to the variables <span class="math inline">\(x_j\)</span>, and then propose a new set of <span class="math inline">\(x_j\)</span>’s. However, we can’t inspect all these values quantumly, so we need to do something else.</p>
<p>In the paper, they demonstrate an approach that requires a few copies of the current state <span class="math inline">\(|x^{(t)}\rangle\)</span>, but will produce a new state <span class="math inline">\(|x^{(t+1)}\rangle\)</span> such that (with objective/loss function <span class="math inline">\(f\)</span>):</p>
\begin{align*}
    |x^{(t+1)}\rangle = |x^{(t)}\rangle - \eta |\nabla
    f\left(x^{(t)}\right)\rangle
\end{align*}
<p>for some step size <span class="math inline">\(\eta\)</span>. That is, it’s a step in the (hopefully) right direction, as per normal gradient descent!</p>
<p>So one direction to take this paper would be to build a “fully quantum” neural network like so:</p>
<div style="text-align: center">
<div class="figure">
<img src="../images/fully-quantum-nn.png" alt="Figure 5. Fully-quantum neural network. The inputs and the weights are quantum states; and the activation function U is unitary." />
<p class="caption">Figure 5. Fully-quantum neural network. The inputs and the weights are quantum states; and the activation function <span class="math inline">\(U\)</span> is unitary.</p>
</div>
</div>
<p>where we make the weights quantum states, and the weights are multiplied onto the inputs as a dot-product. This would require that the weight state is the same size as the input state; but that should be possible because we’re the ones building the network structure.</p>
<p>We could then not worry about learning unitary matrices, and analogously to standard neural networks, just pick some unitary <span class="math inline">\(U\)</span> that “works well” in practice, maybe by just defining quantum analogues of some common activation functions, perhaps say the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> or <a href="https://arxiv.org/abs/1511.07289">ELU</a>.</p>
<p>Overall I think that the quantum gradient descent algorithm should be useful for training neural networks, and maybe some cool things will come from it. There are some natural direct extensions of this work; namely to extend the implementation to <a href="http://sebastianruder.com/optimizing-gradient-descent/index.html">the more practical variations</a>.</p>
<h2 id="quantum-autoencoders-for-efficient-compression-of-quantum-data">1612.02806 - Quantum autoencoders for efficient compression of quantum data</h2>
<p>This paper came out only a few days after the Wan et al paper that we covered above, that also discussed autoencoders, so I thought it was worth a glance to see if this team did things differently.</p>
<p>This paper again takes the approach of not concerning itself with weights and instead focuses on learning a good unitary matrix <span class="math inline">\(U\)</span> with a specific cost function.</p>
<p>They take a different approach in how they build their unitaries. Here they have a “programmable” quantum circuit, where they consider the parameters defining this circuit as the ones that can be trained. Given that these parameters are classical, and loss function they calculate is classical, no special optimisation techniques are needed.</p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>It appears that the building blocks are being put together to start doing some serious work on quantum machine learning/quantum deep networks. <a href="http://web.physics.ucsb.edu/~martinisgroup/">Google</a> and <a href="http://blogs.microsoft.com/next/2016/11/20/microsoft-doubles-quantum-computing-bet/#sm.001umo7pp17ozfkwvl81eu0szc8pf">Microsoft</a> are already heavily investing in quantum computers, Google in particular has something it calls the <a href="https://plus.google.com/+QuantumAILab">“Quantum A.I. Lab”</a>, and there are even independent <a href="http://rigetti.com/">quantum computer manufacturing groups</a>.</p>
<p>It seems like there are lots of options on which way to direct efforts in the quantum ML world, and with these recent developments on quantum ML techniques, the time appears to be right to be getting into quantum deep learning!</p>
<h2 id="appendix">Appendix <a id="appendix"></a></h2>
<p>More interesting quantum machine learning papers:</p>
<ul>
<li><a href="https://scirate.com/arxiv/1611.09347">1611.09347 - Quantum Machine Learning</a> (most recent review)</li>
<li><a href="https://scirate.com/arxiv/1610.08251">1610.08251 - Quantum-enhanced machine learning</a></li>
<li><a href="https://scirate.com/arxiv/1605.05370">1605.05370 - Training A Quantum Optimizer</a></li>
<li><a href="https://scirate.com/arxiv/1603.08675">1603.08675 - Quantum Recommender Systems</a></li>
<li><a href="https://scirate.com/arxiv/1512.06069">1512.06069 - Demonstration of quantum advantage in machine learning</a></li>
<li><a href="https://scirate.com/arxiv/1512.02900">1512.02900 - Advances in quantum machine learning</a></li>
<li><a href="https://scirate.com/arxiv/1611.08104">1611.08104 - Quantum Enhanced Inference in Markov Logic Networks</a></li>
<li><a href="https://scirate.com/arxiv/1609.02542">1609.02542 - Quantum-assisted learning of graphical models with arbitrary pairwise connectivity</a></li>
<li><a href="https://scirate.com/arxiv/1609.06935">1609.06935 - Quantum Neural Machine Learning - Backpropagation and Dynamics</a></li>
<li><a href="https://scirate.com/arxiv/1606.02318">1606.02318 - Solving the Quantum Many-Body Problem with Artificial Neural Networks</a></li>
<li>even more: <a href="https://scirate.com/search?utf8=%E2%9C%93&amp;q=quantum+machine+learning">SciRate - quantum machine learning</a>!</li>
</ul>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Here <em>efficient</em> means that the problem is in the complexity class caled <span class="math inline">\(\textbf{P}\)</span>. Problems that are efficient for quantum computers are in the complexity class <span class="math inline">\(\textbf{BQP}\)</span>. One of the main outstanding questions in the field is “Are quantum computers more powerful than classical ones?” and this can be phrased as comparing the class <span class="math inline">\(\textbf{P}\)</span> and <span class="math inline">\(\textbf{BQP}\)</span>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><em>“Something like” x</em> here is a very informal term for the more formal statement that the running time is <span class="math inline">\(O(x)\)</span>. See <a href="https://en.wikipedia.org/wiki/Big_O_notation">Big O Notation</a> for more.<a href="#fnref2">↩</a></p></li>
</ol>
</div>
]]></summary>
</entry>
<entry>
    <title>50 bad maths and programming jokes</title>
    <link href="https://silky.github.io/posts/2016-02-22-bad-maths-and-programming-jokes.html" />
    <id>https://silky.github.io/posts/2016-02-22-bad-maths-and-programming-jokes.html</id>
    <published>2016-02-22T00:00:00Z</published>
    <updated>2016-02-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="info">
    Posted on February 22, 2016
    
        by Noon van der Silk
    
</div>

<p>For the past few months at work we’ve been putting up a Chalkboard in front of the office with jokes on it.</p>
<div class="figure">
<img src="/images/brunch-and-bound.png" />

</div>
<p>Today marks the 50th joke, so to celebrate I’m writing up the complete list. Most of the jokes here were ones we made up without looking at the internet; but occasionally, in an effort to have two new jokes every day, we picked some classics.</p>
<blockquote>
<p>Q: What kind of parade did the astronauts throw for the computers after the mission?</p>
<p>A: A Turing tape parade!</p>
</blockquote>
<blockquote>
<p>Q: Why was the maths book sad?</p>
<p>A: It had too many problems.</p>
</blockquote>
<blockquote>
<p>Q: What did the AI say to the category theorist?</p>
<p>A: Does not commute!</p>
<p><small>(from <a href="https://andy.kitchen">Andy Kitchen</a>)</small></p>
</blockquote>
<blockquote>
<p>Q: How did the OR programmer solve a MIP while also eating?</p>
<p>A: By using a brunch and bound technique.</p>
</blockquote>
<blockquote>
<p>[“hip”, “hip”]</p>
</blockquote>
<blockquote>
<p>Q: What do measure theorists and programmers have in common?</p>
<p>A: They both enjoy continuous integration.</p>
</blockquote>
<blockquote>
<p>Old mathematicians never die, they just lose some of their functions.</p>
</blockquote>
<blockquote>
<p>Q: Why did the computer keep sneezing?</p>
<p>A: It had a virus.</p>
</blockquote>
<blockquote>
<p>Q: Why wasn’t the complex beer successful?</p>
<p>A: People had trouble ordering it!</p>
</blockquote>
<blockquote>
<p>Q: Why did the functional programmer return her TV?</p>
<p>A: Because it was immutable.</p>
</blockquote>
<blockquote>
<p>Q: What do ruby and librarians have in common?</p>
<p>A: They both have explicit return policies.</p>
</blockquote>
<blockquote>
<p>A shepherd was out in the field counting her sheep; she counted 96 but when she rounded them up she had 100.</p>
<p><small>(from <a href="http://www.twolostboys.com.au/">Two Lost Boys</a>)</small></p>
</blockquote>
<blockquote>
<p>Q: Why was the computer owner so successful at sheep husbandry?</p>
<p>A: She had excellent RAM.</p>
</blockquote>
<blockquote>
<p>Q: Why couldn’t the formal system complete it’s homework?</p>
<p>A: It was trying to be consistent.</p>
</blockquote>
<blockquote>
<p>404: Joke Not Found.</p>
</blockquote>
<blockquote>
<p>Q: How does a lumberjack mathematician cut down trees?</p>
<p>A: With her Axiom.</p>
</blockquote>
<blockquote>
<p>Q: Why did the programmer go to her bookshelf before leaving her house?</p>
<p>A: She needed to get her keys from the dictionary.</p>
</blockquote>
<blockquote>
<p>Q: Why don’t you want to fight an OR consultant?</p>
<p>A: They are experts at duals.</p>
</blockquote>
<blockquote>
<p>Q: What did the Linux system administer for the programmer’s head cold?</p>
<p>A: Sudo ephedrine</p>
</blockquote>
<blockquote>
<p>Q: How did the physicist fix her car when it was failing intermittently?</p>
<p>A: She used statistical mechanics!</p>
</blockquote>
<blockquote>
<p>Q: Why was the bad python programmer so rich?</p>
<p>A: Because everytime his code failed he got a raise.</p>
</blockquote>
<blockquote>
<p>Q: What do python programmers and event planners have in common?</p>
<p>A: They both like to decorate functions.</p>
</blockquote>
<blockquote>
<p>Q: Why is 0 the boss?</p>
<p>A: Because no other number can go above it!</p>
</blockquote>
<blockquote>
<p>Q: What did the mathematician say when they discovered a new prime number?</p>
<p>A: That’s odd.</p>
</blockquote>
<blockquote>
<p>Q: Why did the low-rank matrix go to the psychologist?</p>
<p>A: Because it was having an identity crisis!</p>
</blockquote>
<blockquote>
<p>Q: What is a floating point numbers favourite type of tennis?</p>
<p>A: Doubles!</p>
</blockquote>
<blockquote>
<p>Q: What does a blender and the Kalman filter hav in common?</p>
<p>A: They both perform a smoothing function!</p>
</blockquote>
<blockquote>
<p>Q: What is the mathematicians favourite kitchen item?</p>
<p>A: Derivasieve.</p>
</blockquote>
<blockquote>
<p>Q: Why don’t elephants use computers?</p>
<p>A: Scared of the mouse.</p>
</blockquote>
<blockquote>
<p>Q: Why was the OR consultant unwell?</p>
<p>A: She want on a benders.</p>
</blockquote>
<blockquote>
<p>Q: What is a statisticians favourite genre of music?</p>
<p>A: Drum and Bayes.</p>
</blockquote>
<blockquote>
<p>Q: What is a pet store operatores favourite state in a multiplayer game?</p>
<p>A: The Parrot optimal state.</p>
</blockquote>
<blockquote>
<p>Q: What is the enterprise java programmers favourite business book?</p>
<p>A: Scalaing up!</p>
</blockquote>
<blockquote>
<p>Q: What function is a tree hugger most concerned by?</p>
<p>A: <span class="math inline">\(\log(n)\)</span>.</p>
</blockquote>
<blockquote>
<p>Q: What is a garbologists favourite optimisation problem?</p>
<p>A: Bin packing.</p>
</blockquote>
<blockquote>
<div class="figure">
<img src="/images/giraph.png" />

</div>
</blockquote>
<blockquote>
<p>Q: What is a choirs favourite design pattern?</p>
<p>A: The <em>Sing</em>leton pattern!</p>
</blockquote>
<blockquote>
<p>Q: What do you call a mathematician that has lots of statues in her garden?</p>
<p>A: Polygnomial.</p>
</blockquote>
<blockquote>
<p>Q: What do fashion designers and Haskell programmers have in common?</p>
<p>A: They love pattern matching!</p>
</blockquote>
<blockquote>
<p>Q: How did the mathematician impress at the dance party?</p>
<p>A: By showing off her step function!</p>
</blockquote>
<blockquote>
<p>No joke provided; the Curry-Howard isomorphism allows us to generate a programming joke from the maths joke.</p>
</blockquote>
<blockquote>
<p>Q: Why was the mathematician unhappy when she turned 24?</p>
<p>A: She now had a lot of factors to consider.</p>
</blockquote>
<blockquote>
<p>Q: Why was the programmer so poor?</p>
<p>A: Syn<em>tax</em>.</p>
</blockquote>
<blockquote>
<p>Q: Why was the ML programmer late to the conference?</p>
<p>A: She spent too much time in the “train” stage.</p>
</blockquote>
<blockquote>
<p>Q: What number is good value?</p>
<p>A: 241</p>
<p>Exercise: What number is best value?</p>
</blockquote>
<blockquote>
<p>Q: Why couldn’t the python programmer get into her house?</p>
<p>A: Key error.</p>
</blockquote>
<blockquote>
<p>Q: How did the programmer get out of the deep end of the pool?</p>
<p>A: She made a pull request!</p>
</blockquote>
<blockquote>
<p>Q: Why was the ML researcher tired of shopping?</p>
<p>A: She was overfitting.</p>
</blockquote>
<blockquote>
<p>Q: How did the programmer get to the bottom of the ocean?</p>
<p>A: By sub-routine!</p>
</blockquote>
<blockquote>
<p>Q: How do you order citrus?</p>
<p>A: Use the real number lime.</p>
</blockquote>
]]></summary>
</entry>
<entry>
    <title>Building a windows-executable Haskell program with stack and AppVeyor</title>
    <link href="https://silky.github.io/posts/2016-01-05-build-windows-haskell-app-with-stack-and-appveyor.html" />
    <id>https://silky.github.io/posts/2016-01-05-build-windows-haskell-app-with-stack-and-appveyor.html</id>
    <published>2016-01-05T00:00:00Z</published>
    <updated>2016-01-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="info">
    Posted on January  5, 2016
    
        by Noon van der Silk
    
</div>

<p>In this post we’ll see how to setup a CI build that generates Windows executables for stack-based Haskell projects.</p>
<p>So imagine you love Haskell and you have written a Haskell program, you’ve hosted it on <a href="https://github.com">GitHub</a> and you’ve dilligently set up a CI build on <a href="https://travis-ci.org">Travis</a> that uses <a href="https://github.com/commercialhaskell/stack">stack</a> to build/test/etc.</p>
<p>Now, because stack is great and life is good, people can build your project from the source largely without issue. But suppose now that you’d like to provide Windows binaries for download. It so happens that we live in an age where this is completely automatable! Let’s see how.</p>
<p>The essence of my approach is to use the CI system <a href="http://www.appveyor.com/">AppVeyor</a> (essentially like a Travis for Windows), and the following <code>appveyor.yml</code> file (full file below, I’ll go through details next):</p>
<div class="sourceCode"><table class="sourceCode yaml numberLines"><tr class="sourceCode"><td class="lineNumbers"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="sourceCode"><pre><code class="sourceCode yaml"><span class="fu">build:</span> off

<span class="fu">before_build:</span>
<span class="kw">-</span> <span class="fu">curl -ostack.zip -L --insecure http:</span>//www.stackage.org/stack/windows-i386
<span class="kw">-</span> 7z x stack.zip stack.exe
<span class="kw">-</span> <span class="fu">sed -i &#39;s/git@github.com:</span>/https:\/\/github.com\//&#39; .gitmodules
<span class="co"># Appveyor doesn&#39;t clone recursively.</span>
<span class="kw">-</span> git submodule update --init --recursive

<span class="fu">skip_tags:</span> true

<span class="fu">build_script:</span>
<span class="co"># Suppress output from stack setup, as there is a lot and it&#39;s not necessary.</span>
<span class="kw">-</span> stack setup --no-terminal &gt; nul
<span class="kw">-</span> stack build --only-snapshot --no-terminal
<span class="kw">-</span> stack --local-bin-path . install haskmas
<span class="co"># Set a magical environment variable</span>
<span class="kw">-</span> <span class="fu">cmd:</span> for /f %%i in (&#39;stack exec -- haskmas -v&#39;) do set HASKMAS_VERSION=%%i

<span class="fu">artifacts:</span>
<span class="kw">-</span> <span class="fu">path:</span> haskmas.exe

<span class="co"># Auto-deploy</span>
<span class="fu">deploy:</span>
  <span class="kw">-</span> <span class="fu">provider:</span> GitHub
    <span class="fu">tag:</span> <span class="st">&#39;haskmas-$(HASKMAS_VERSION)&#39;</span>
    <span class="fu">release:</span> <span class="st">&#39;Release haskmas-$(HASKMAS_VERSION)&#39;</span>
    <span class="fu">auth_token:</span>
      <span class="fu">secure:</span> FZXhwa1ucQwyFtswv/XNUJkclAxoz4YGGu69dSOEEkwG7Rlh/Gho66SJtOUJ57kN
    <span class="fu">artifact:</span> haskmas.exe
    <span class="fu">on:</span>
      <span class="fu">branch:</span> master</code></pre></td></tr></table></div>
<p>Details:</p>
<ul>
<li>Line 1 disables the <em>standard</em> build process which would use MSBuild.</li>
<li>Lines 3-8 obtain the stack executable, and also perform a small hack which converts my <code>ssh</code>-based git submodules to <code>https</code>-based ones, that can be cloned without needing to mess about with ssh keys.</li>
<li>Line 10 prevents AppVeyor from building when it sees a new tag (later in the script we end up making a new tag when we push a release)</li>
<li>Lines 12-16 perform the typical stack build, and also install the <code>haskmas.exe</code> file that we will mark as an artifact</li>
<li>Line 18 is a magic command that sets the environment variable <code>HASKMAS_VERSION</code> to the value of the output of the command <code>stack exec -- haskmas -v</code>. This is my “hack” to obtain the cabal-version of the <code>haskmas</code> library, which I use as part of the tag that gets released on the <a href="https://github.com/silky/haskmas/releases">GitHub releases page</a></li>
<li>Line 21 simply marks the <code>haskmas.exe</code> as an artifact; this means AppVeyor will hang on to it after the build completes.</li>
<li>and finally, lines 24-32 specify that, for each build that completes, AppVeyor should push a release with the tag <code>haskmas-&lt;cabal_version_of_haskmas&gt;</code> to the GitHub releases page! (Note: probably we would want to be a bit more elaborate about when we push to the releases page; making sure that we include proper release notes, etc.)</li>
</ul>
<p>You can see from the releases on the <code>haskmas</code> project istelf that I fumbled around with this setup a bit. Mostly I observed that the manual release process on AppVeyor doesn’t quite operate the way I’d’ve <a href="https://github.com/appveyor/ci/issues/593">hoped</a>; but in any case AppVeyor is a pretty convenient service; it’s very nice to see something for Windows in this space.</p>
<p>I based my configuration off of the one for the <a href="https://github.com/commercialhaskell/stack/blob/master/appveyor.yml">stack tool itself</a>. Note that there is some mention of caching in there that might speed up the build (my build takes ~17 minutes on AppVeyor compared to ~2 minutes on travis).</p>
<p>All-in-all, technical details about automatically pushing releases aside, AppVeyor+stack is a really nice way to build Windows binaries from exactly the same source as your linux binaries. The only outstanding item to do is to combine artifacts from travis and AppVeyor into a single entity that can be released dually; but on the other hand tooling could always be written to perform this semi-manually, from your working computer, when you are ready to release.</p>
<p>The repo is here: <a href="https://github.com/silky/haskmas">haskmas</a>. As a side benefit, I made the haskmas program take command line arguments to control how it operates! So now you don’t need to compile it to get a tree of arbitrary depth!</p>
]]></summary>
</entry>
<entry>
    <title>Happy Haskmas!</title>
    <link href="https://silky.github.io/posts/2015-12-18-happy-haskmas.html" />
    <id>https://silky.github.io/posts/2015-12-18-happy-haskmas.html</id>
    <published>2015-12-18T00:00:00Z</published>
    <updated>2015-12-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="info">
    Posted on December 18, 2015
    
        by Noon van der Silk
    
</div>

<p>At the last <a href="http://www.meetup.com/Melbourne-Haskell-Users-Group/events/222203592/">Melbourne Haskell Meetup</a> we got into the spirit by making <a href="https://github.com/imccoy/xmast">Christmas trees in Haskell</a>.</p>
<p>However, I recently have access to a 3D printer, and I’ve long wanted an excuse to try and use <a href="https://github.com/colah/ImplicitCAD">ImplicitCAD</a>, so I set about trying to make a 3D version of <a href="https://github.com/sordina">Lyndon</a>’s <a href="http://www.meetup.com/Melbourne-Haskell-Users-Group/photos/26573949/">logo</a>.</p>
<p>So of course, I love <a href="https://github.com/commercialhaskell/stack">stack</a> I created a new <code>simple</code> project with <code>stack new</code> and got started.</p>
<p>It turns out that <code>ImplicitCAD</code> has a pretty nice and reasonably intuitive interface (similar to the code that one would write into <a href="http://www.openscad.org/">OpenSCAD</a>).</p>
<p>I built the 3D version of the logo by moving around rectangles, and by extruding a hand-drawn shape.</p>
<p>The cool thing about generating this image in fully-feature programming language is that I can build a tree of any size I like!</p>
<p>Here’s the (pretty verbose) code that gets me a tree of arbitrary depth:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">ntree ::</span> <span class="dt">Integer</span> <span class="ot">-&gt;</span> <span class="dt">SymbolicObj3</span>
ntree n <span class="fu">=</span> finalObj
  <span class="kw">where</span>
      dec     <span class="fu">=</span> <span class="fl">0.8</span>
      ratios  <span class="fu">=</span> <span class="dv">0</span> <span class="fu">:</span> [dec<span class="fu">^</span>j <span class="fu">|</span> j <span class="ot">&lt;-</span> [<span class="dv">0</span><span class="fu">..</span>(n<span class="fu">-</span><span class="dv">2</span>)]]
      <span class="co">-- build up logo structure</span>
      ((lx, ly, lz), objs) <span class="fu">=</span> foldl f ((<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), []) (zip [<span class="dv">0</span><span class="fu">..</span>(n<span class="fu">-</span><span class="dv">1</span>)] ratios)
      <span class="co">-- position of logos</span>
      (x,y,z) <span class="fu">=</span> (<span class="dv">40</span>, <span class="dv">4</span>, <span class="dv">0</span>)
<span class="ot">      f ::</span> ((ℝ, ℝ, ℝ), [<span class="dt">SymbolicObj3</span>]) <span class="ot">-&gt;</span> (<span class="dt">Integer</span>, <span class="dt">Float</span>) <span class="ot">-&gt;</span> ((ℝ, ℝ, ℝ), [<span class="dt">SymbolicObj3</span>])
      f ((x&#39;, y&#39;, z&#39;), xs) (j, r) <span class="fu">=</span>
                <span class="kw">let</span> newPos <span class="fu">=</span> (x&#39; <span class="fu">+</span> r<span class="fu">*</span>x, y&#39; <span class="fu">+</span> r<span class="fu">*</span>y, z&#39; <span class="fu">+</span> r<span class="fu">*</span>z)
                    s      <span class="fu">=</span> dec <span class="fu">^</span> j
                    loc    <span class="fu">=</span> <span class="kw">if</span> (even j) <span class="kw">then</span> <span class="dt">R</span> <span class="kw">else</span> <span class="dt">L</span>
                    obj3   <span class="fu">=</span> translate newPos <span class="fu">$</span> scale (s, s, s) (logoBauble loc)
                 <span class="kw">in</span> (newPos, obj3 <span class="fu">:</span> xs)
      <span class="co">-- star</span>
      (a,b,c)   <span class="fu">=</span> (<span class="fl">40.5</span>, <span class="fl">24.5</span>, <span class="dv">0</span>)
      starScale <span class="fu">=</span> dec <span class="fu">**</span> (fromIntegral (n<span class="fu">-</span><span class="dv">3</span>))
      posScale  <span class="fu">=</span> dec <span class="fu">**</span> (fromIntegral n)
      starObj   <span class="fu">=</span> translate (lx <span class="fu">+</span> (posScale <span class="fu">*</span> a), ly <span class="fu">+</span> (posScale <span class="fu">*</span> b), lz <span class="fu">+</span> (posScale <span class="fu">*</span> c))
                    <span class="fu">$</span> scale (starScale, starScale, starScale) star
      finalObj <span class="fu">=</span> union (starObj <span class="fu">:</span> objs)</code></pre></div>
<p>Running this with <span class="math inline">\(n = 5\)</span> gives result in the following render in OpenSCAD:</p>
<div class="figure">
<img src="/images/tree-5.png" />

</div>
<p>So there you have it! You can view the source code here: <a href="https://github.com/silky/haskmas">github.com/silky/haskmas</a> or download a ready-to-print STL from <a href="http://www.thingiverse.com/thing:1187442">Thingiverse</a>.</p>
<p>Have a happy Haskmas! :)</p>
]]></summary>
</entry>
<entry>
    <title>Super Reference! Web Haskell-based reference management</title>
    <link href="https://silky.github.io/posts/2015-10-14-super-reference-haskell-reference-manager.html" />
    <id>https://silky.github.io/posts/2015-10-14-super-reference-haskell-reference-manager.html</id>
    <published>2015-10-14T00:00:00Z</published>
    <updated>2015-10-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="info">
    Posted on October 14, 2015
    
        by Noon van der Silk
    
</div>

<p>Today I’m announcing a (very) alpha version of my web-based reference management system, <a href="https://github.com/silky/super-reference">super-reference</a>!</p>
<p>Super-reference is a system which:</p>
<ol style="list-style-type: decimal">
<li>Reads a bibtex file and lets you search the bibtex entries in it</li>
<li>Provides an interface to open associated PDFs or visit associated links</li>
<li>Maintains a list of ‘currently reading’ PDFs by synching a folder (for me, a dropbox folder)</li>
</ol>
<p>Here’s a screenshot of the main (only) page:</p>
<div class="figure">
<img src="https://raw.github.com/silky/super-reference/master/another_screenshot.png" />

</div>
<p>Super-reference is built around my workflow for getting the latest papers.</p>
<p>A high-level overview of my workflow is:</p>
<ol style="list-style-type: decimal">
<li>Find an interesting paper</li>
<li>Obtain the pdf file for this paper, (preferably from <a href="https://arxiv.org">arXiv</a>)</li>
<li>Save this pdf on my computer, and get the <code>bibtex</code> data for the file somehow</li>
<li>At some point in the future, be interested in finding this PDF by searching for the title, or browsing through a list</li>
</ol>
<p>All the while I’d like to maintain a list of “papers I’m currently reading” and have them available on my tablet.</p>
<p>Items 1-3 take place <em>outside</em> of super-reference. My workflow for finding new papers and saving them on my computer is a little specialised, but happily you don’t need to follow this protocol if you want to get utility out of super-reference — you simply need a <code>bibtex</code> file to point it at.</p>
<p>Here’s my new paper obtaining workflow (yours may be different):</p>
<ol style="list-style-type: decimal">
<li>Browse <a href="https://scirate.com">SciRate</a> regularly, and “scite” interesting papers</li>
<li>Periodically run <a href="https://github.com/silky/scirate3_scraper">scirate3_scaper</a> to bring down all the papers I’ve scited recently, and their bibtex entries</li>
<li>Combine all these bibtex entries ino my <em>One True</em> bibtex file</li>
</ol>
<h2 id="using-super-reference">Using super-reference</h2>
<p>Let’s suppose now you have a <em>One True</em> bibtex file called <code>all.bib</code>.</p>
<p>Grab yourself down a copy of the repository by following the <a href="https://github.com/silky/super-reference/#installation">instructions</a> to configure it to point to this bibtex file.</p>
<p>Once you have the environment set up you can run a dev server with <code>yesod devel</code>. You will then be able to visit the website! Supposing you pointed the config file at your <code>all.bib</code> file, you will then be looking at all your interesting papers! If there’s a link associated with the entry, a link will be displayed, and if there is a PDF file, clicking on the entry name will open the PDF (with it’s location given by the <code>file</code> entry in the relevant bibtex entry).</p>
<p>You can click the <code>star</code> link to move the PDF up into the “currently reading” section. (Note: At the moment there is no visual indication of this, but it will be shown next time the page loads.)</p>
<h2 id="improvements">Improvements</h2>
<p>Currently there is a lot of outstanding work to do on super-reference; but it’s in a working state for me, so I thought I should release it.</p>
<p>If you have any feature requests/improvements/etc feel free to log an issue (or a pull request) over at the repository: <a href="https://github.com/silky/super-reference">super-reference</a>.</p>
]]></summary>
</entry>
<entry>
    <title>Versioned LyX documents</title>
    <link href="https://silky.github.io/posts/2015-10-01-Makefile-for-versioned-lyx.html" />
    <id>https://silky.github.io/posts/2015-10-01-Makefile-for-versioned-lyx.html</id>
    <published>2015-10-01T00:00:00Z</published>
    <updated>2015-10-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="info">
    Posted on October  1, 2015
    
        by Noon van der Silk
    
</div>

<p>Oftentimes one needs to write a document with math symbols in it. The standard tool of choice is some variant of TeX, either writing it online in one of the growing-list of collaborative editors:</p>
<ul>
<li><a href="https://www.overleaf.com/">Overleaf</a></li>
<li><a href="https://www.sharelatex.com/">ShareLaTeX</a></li>
<li><a href="https://www.authorea.com/">Authorea</a></li>
</ul>
<p>But one program, that runs locally, that I can’t stop using is <a href="http://www.lyx.org/">LyX</a>.</p>
<p>I really like LyX because of the “What-you-see-is-(pretty much)-what-you-get” nature of it.</p>
<p>One thing I wanted to share was a small technique that I used to get a version number in all of the pdfs that I generated from my LyX documents. The idea was that when I sent my document to obtain feedback from various interested parties, I could easily see which version they had commented on.</p>
<p>What I wanted as a footer that would be included on every page, that contained the version number.</p>
<p>The approach is:</p>
<ol style="list-style-type: decimal">
<li>In the “LaTeX Preamble” setting of the LyX document, include (something like)</li>
</ol>
<pre><code>\usepackage{fancyhdr}
\pagestyle{fancy}
\cfoot{\tiny{|VERSION|}}
\rfoot{\thepage}
\rhead{}</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Build your LyX documents by the command line with a <code>Makefile</code>. My <code>Makefile</code> looks like so:</li>
</ol>
<pre><code>BUILD_NUMBER_FILE := build-number.txt
BUILD_NUMBER      := $(shell cat $(BUILD_NUMBER_FILE))
BUILD_DATE        := $(shell date +%d%b%Y)
VER_STRING        := $(BUILD_DATE)-build$(BUILD_NUMBER)
LYXFILE           := coolness
TEMPDIR           := /tmp

all: pdf

# Switch in the new version number, compile with LyX and
# bring it here.
pdf: buildnumber
	sed &quot;s/|VERSION|/$(VER_STRING)/g&quot; $(LYXFILE).lyx &gt;$(TEMPDIR)/$(LYXFILE).lyx
	lyx -e pdf2 $(TEMPDIR)/$(LYXFILE).lyx
	cp $(TEMPDIR)/$(LYXFILE).pdf .

# Build number file. Increment each build.
buildnumber:
	@if ! test -f $(BUILD_NUMBER_FILE); then echo 0 &gt; $(BUILD_NUMBER_FILE); fi
	@echo $$(($$(cat $(BUILD_NUMBER_FILE)) + 1)) &gt; $(BUILD_NUMBER_FILE)</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Build with the <code>make</code> command and profit!</li>
</ol>
<p>Note that there is a <code>build-number.txt</code> file that is incremented on each build, so you don’t need to do that manually.</p>
<p>I’ve put together a sample project <a href="https://gist.github.com/silky/9accc6e6a5dbbc029669">here</a>, so you can clone that gist and type <code>make</code> and see it in action!</p>
]]></summary>
</entry>
<entry>
    <title>GHCi Colouriser</title>
    <link href="https://silky.github.io/posts/2015-09-22-ghci-colouriser.html" />
    <id>https://silky.github.io/posts/2015-09-22-ghci-colouriser.html</id>
    <published>2015-09-22T00:00:00Z</published>
    <updated>2015-09-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="info">
    Posted on September 22, 2015
    
        by Noon van der Silk
    
</div>

<p>Behold, a colourful GHCi (<a href="https://github.com/silky/ghci-color">install it for yourself</a>):</p>
<div class="figure">
<img src="https://raw.github.com/silky/ghci-color/master/cap2.png" />

</div>
<p>This is done by using a simple little <code>sed</code> wrapper around the ordinary <code>ghci --interactive</code>, see <a href="https://github.com/silky/ghci-color/blob/master/ghci-color">here</a> for the script.</p>
<p>Note in particular the quirk that we must capture any <code>SIGINT</code> that gets sent through and discard it, when we are invoking <code>sed</code>, otherwise <code>sed</code> itself will quit and our GHCi session will break.</p>
<p>You can use it <code>cabal repl</code> (being mindful of <a href="https://github.com/haskell/cabal/issues/1905">this bug</a>) by using <code>cabal repl --with-ghc=ghci-color</code>. I’ve bound an alias <code>repl</code> to this.</p>
<p>(Note: original credit for this goes to <a href="https://github.com/rhysd/">rhysd</a>)</p>
]]></summary>
</entry>
<entry>
    <title>Fresh Prince of Bell Pair</title>
    <link href="https://silky.github.io/posts/2015-05-29-fresh-prince-of-bell-pair.html" />
    <id>https://silky.github.io/posts/2015-05-29-fresh-prince-of-bell-pair.html</id>
    <published>2015-05-29T00:00:00Z</published>
    <updated>2015-05-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="info">
    Posted on May 29, 2015
    
        by Noon van der Silk
    
</div>

<p>Written for my Masters completion talk. <a href="https://vimeo.com/129394232">Video</a>.</p>
<h2 id="lyrics">Lyrics</h2>
<blockquote>
<p>now this is a story, all about how</p>
</blockquote>
<blockquote>
<p>my life got spun, both up and down</p>
</blockquote>
<div class="figure">
<img src="/images/p1.png" />

</div>
<blockquote>
<p>and i’d like to take a minute, just sit right there</p>
</blockquote>
<blockquote>
<p>and i’ll tell you how i became the prince, of the state called <a href="http://en.wikipedia.org/wiki/Bell_state">bell pair</a></p>
</blockquote>
<div class="figure">
<img src="/images/p2.png" />

</div>
<blockquote>
<p>in programming languages, born and raised, on the computer, was where i spent most of my days</p>
</blockquote>
<blockquote>
<p>typing out, compiling, building some tools, and all surfing the internet looking for news</p>
</blockquote>
<blockquote>
<p>when a couple of guys who seemed kinda fine, started making trouble in polynomial time!</p>
</blockquote>
<div class="figure">
<img src="/images/p3.png" />

</div>
<blockquote>
<p>they got in one little fight (BQP vs BPP) and everyone got scared,</p>
</blockquote>
<div class="figure">
<img src="/images/p4.png" />

</div>
<blockquote>
<p>someone said: we’re going to build a quantum computer, so you better be prepared!</p>
</blockquote>
<div class="figure">
<img src="/images/p5.png" />

</div>
]]></summary>
</entry>
<entry>
    <title>Introduction to quantum computing and the surface code</title>
    <link href="https://silky.github.io/posts/2014-09-09-intro-to-qc-and-the-surface-code.html" />
    <id>https://silky.github.io/posts/2014-09-09-intro-to-qc-and-the-surface-code.html</id>
    <published>2014-09-09T00:00:00Z</published>
    <updated>2014-09-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="info">
    Posted on September  9, 2014
    
        by Noon van der Silk
    
</div>

<p><em>references: <a href="https://scirate.com/arxiv/0904.2557">arXiv:0904.2557</a>, <a href="https://scirate.com/arxiv/1208.0928">arXiv:1208.0928</a></em></p>
<p>(This is the content of a talk I gave to other students in our department, most of whom have no background in quantum computing; hence the introduction and lightness on details of the surface code.)</p>
<p>Before talking about the surface code, I’ll introduce the fundamentals of quantum computing.</p>
<p><strong>Quantum computing</strong>: The evaluation of quantum circuits in polynomial time.</p>
<h2 id="background-on-quantum-computing">Background on quantum computing</h2>
<p><strong>State</strong> A <em>state</em> is a <span class="math inline">\(d\)</span>-dimensional vector in a complex Hilbert space <span class="math inline">\(\mathcal{H}^d\)</span>. We require that the states are normalised to unity.</p>
<p><strong>Qubit</strong> A <em>qubit</em> is a “two-level system”. This means it is a state with <span class="math inline">\(d = 2\)</span>.</p>
<p>We are doing quantum mechanics, so we use bras and kets for vectors, and we distinguish some particular bases,</p>
<p><span class="math display">\[ \begin{aligned}
    |0\rangle &amp;= \left( \begin{array}{c}
        1 \\ 0
    \end{array} \right), \\
    |1\rangle &amp;= \left( \begin{array}{c}
        0 \\ 1
    \end{array} \right).
\end{aligned} \]</span></p>
<p>Note that these two vectors form a basis for <span class="math inline">\(\mathcal{H}^2\)</span>. We call this basis the <em>standard</em> basis. All bases here will are assumed to be orthonormal.</p>
<p>Written in the standard basis an arbitrary qubit is then given by</p>
<p><span class="math display">\[ \begin{aligned}
    |\psi\rangle = \alpha |0\rangle + \beta |1\rangle.
\end{aligned} \]</span></p>
<p>For a ket <span class="math inline">\(|\psi\rangle\)</span> the bra is given by <span class="math inline">\(\langle\psi| = (|\psi\rangle)^\dagger\)</span>, where <span class="math inline">\(\dagger\)</span> means take the conjugate tranpose. So we write the inner product as <span class="math inline">\(\langle \psi|\varphi\rangle\)</span> and the outer product as <span class="math inline">\(|\psi\rangle\langle\varphi|\)</span>.</p>
<p>We will also refer to the so-called <em>Hadamard basis</em>, which is</p>
<p><span class="math display">\[ \begin{aligned}
    |+\rangle &amp;= \frac{1}{\sqrt{2}}\left(|0\rangle + |1\rangle\right), \\
    |-\rangle &amp;= \frac{1}{\sqrt{2}}\left(|0\rangle - |1\rangle\right).
\end{aligned} \]</span></p>
<p><strong>Qubits</strong> We can form a state containing multiple qubits using the tensor product. Verbosely, one may write:</p>
<p><span class="math display">\[ \begin{aligned}
    |0\rangle \otimes |0\rangle \otimes \cdots
\end{aligned} \]</span></p>
<p>but we will shorten this to:</p>
<p><span class="math display">\[ \begin{aligned}
    |00\cdots\rangle &amp;= |0\rangle \otimes |0\rangle \otimes \cdots.
\end{aligned} \]</span></p>
<p>The dimension of the Hilbert space of this state is <span class="math inline">\(\mathcal{H}^{2^n}\)</span>, where <span class="math inline">\(n\)</span> is the number of terms in the tensor product: the number of qubits.</p>
<p><strong>Operator</strong> An <em>operator</em> (sometimes called a <em>gate</em>), M, is a <span class="math inline">\(2^n\times 2^n\)</span>-dimensional matrix with elements in <span class="math inline">\(\mathbb{C}\)</span>. We require that the operators be unitary; i.e. that <span class="math inline">\(M M^\dagger = M^\dagger M = 1\)</span> (this requirement comes from the fact that any such operator should preserve the norm of the states).</p>
<p><strong>A note on notation</strong> Given a multi-qubit state, <span class="math inline">\(|abcd\rangle\)</span>, we can perform a single-qubit operator <span class="math inline">\(M\)</span> on each of these qubits by constructing the appropriate tensor product of matrices, <span class="math inline">\(M \otimes M \otimes M \otimes M\)</span>, and then acting this on the state. We will often write this more concisely as <span class="math inline">\(M_1 M_2 M_3 M_4 |abcd\rangle\)</span>, indicating which qubit the operator acts on.</p>
<p><strong>Pauli Matrices</strong> These play a key role in the surface code. We will define them as</p>
<p><span class="math display">\[ \begin{aligned}
    X &amp;= \left( \begin{array}{cc}
        0 &amp; 1 \\
        1 &amp; 0 \end{array} \right), \\
    Z &amp;= \left( \begin{array}{cc}
        1 &amp; 0 \\
        0 &amp; -1 \end{array} \right), \\
    Y &amp;= \text{i}XZ.
\end{aligned} \]</span></p>
<p>Note that they all square to <span class="math inline">\(1\)</span>. We will only be interested in the action of <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>, so note:</p>
<p><span class="math display">\[ \begin{aligned}
    X|0\rangle &amp;= |1\rangle, \\
    X|1\rangle &amp;= |0\rangle, \\
    Z|+\rangle &amp;= |-\rangle, \\
    Z|-\rangle &amp;= |+\rangle.
\end{aligned} \]</span></p>
<p>And furthermore, we have <span class="math inline">\(XZ = -ZX\)</span> (these operators anticommute).</p>
<p><strong>Measurement</strong> Quantum mechanics prescribes that for an arbitrary qubit (written in the standard basis),</p>
<p><span class="math display">\[ \begin{aligned}
    |\psi\rangle &amp;= \alpha|0\rangle + \beta|1\rangle,
\end{aligned} \]</span></p>
<p>then <em>measuring</em> this qubit gives the state <span class="math inline">\(|0\rangle\)</span> with probability <span class="math inline">\(|\alpha|^2\)</span> and state <span class="math inline">\(|1\rangle\)</span> with probability <span class="math inline">\(|\beta|^2\)</span>.</p>
<p>More generally we can make projective measurements, wherein we project a state <span class="math inline">\(|\psi\rangle\)</span> into a particular basis. It is common to observe that, in terms of eigenvalues and eigenvectors (by the spectral decomposition theorem),</p>
<p><span class="math display">\[ \begin{aligned}
    X &amp;= |+\rangle\langle +| - |-\rangle\langle -|, \\
    Z &amp;= |0\rangle\langle 0| - |1\rangle\langle 1|,
\end{aligned} \]</span></p>
<p>and therefore make statements such as “measure in the <span class="math inline">\(X\)</span> basis”, where this means to project the qubit into the basis <span class="math inline">\(\{|+\rangle, |-\rangle\}\)</span>.</p>
<p><strong>Quantum circuits</strong></p>
<p>An example quantum circuit:</p>
<!--
    Circuit
    |0> ---- X ----- | M_Z |
-->
<div class="figure">
<img src="/images/qcirc_1.jpg" />

</div>
<p>This enacts the operator <span class="math inline">\(X\)</span> on the state <span class="math inline">\(|0\rangle\)</span>, and we “measure” this state at the end of the circuit in the <span class="math inline">\(Z\)</span> basis.</p>
<p>The outcome of this circuit is the state <span class="math inline">\(|1\rangle\)</span> with certainity.</p>
<p>Let <span class="math inline">\(H = \frac{1}{\sqrt{2}} \left( \begin{array}{cc}  1 &amp; 1 \\  1 &amp; -1 \end{array} \right)\)</span> and <span class="math inline">\(C_X = \left( \begin{array}{cccc}  1 &amp; 0 &amp; 0 &amp; 0 \\  0 &amp; 1 &amp; 0 &amp; 0 \\  0 &amp; 0 &amp; 0 &amp; 1 \\  0 &amp; 0 &amp; 1 &amp; 0  \end{array} \right) = \left(  \begin{array}{ccc}  1 &amp; &amp; \\  &amp; 1 &amp; \\  &amp; &amp; X  \end{array} \right)\)</span>.</p>
<p>A more complicated circuit:</p>
<!--
    |0> --- H -- [ C_X ] ------ 
                 |     |
    |0> -------- [     ] ------
-->
<div class="figure">
<img src="/images/qcirc_2.jpg" />

</div>
<p>The outcome of this circuit is the state <span class="math inline">\(|00\rangle + |11\rangle\)</span>. This is infact an <em>entangled</em> state.</p>
<p><strong>Universal quantum computation</strong></p>
<p>Previously I described quantum computation as the evaluation of any circuit in polynomial time i.e. we could choose any unitary operations we like.</p>
<p>Infact, it turns out that, due to a standard theorem (Solovay-Kitaev theorem), that if we can perform the gates <span class="math inline">\(\{H, S, T, C_X\}\)</span>, then we can approximate any unitary matrix (acting on the qubits involved), and hence perform any computation. (It’s not particularly important what the <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> gates are, for our purposes.)</p>
<h2 id="error-correction">Error correction</h2>
<p>A natural question is - “Do we have quantum computers?”, and the answer is, “not yet”. The reason is that quantum states are decliate; they need to be well isolated from the environment; otherwise errors occur.</p>
<h3 id="classical-errors">Classical errors</h3>
<p><em>Idea of encoding</em></p>
<p>Suppose we wanted to worry about errors in bits. We could used the repitition code,</p>
<p><span class="math display">\[ \begin{aligned}
    0 &amp;\mapsto 000, \\
    1 &amp;\mapsto 111.
\end{aligned} \]</span></p>
<p>This exact procedure - copying an arbitrary state - can’t be applied in quantum computing due to the No-Cloning Theorem: There is no matrix <span class="math inline">\(U\)</span> such that for any arbitrary <span class="math inline">\(|\psi\rangle\)</span></p>
<p><span class="math display">\[ \begin{aligned}
    U(|\psi\rangle \otimes |0\rangle) = |\psi\rangle \otimes |\psi\rangle.
\end{aligned} \]</span></p>
<p><em>Proof</em>. Let <span class="math inline">\(|\psi\rangle = \alpha|0\rangle + \beta|1\rangle\)</span> then apply <span class="math inline">\(U\)</span> before and after enacting the tensor product, and note contradiction.</p>
<h3 id="quantum-errors">Quantum errors</h3>
<p>For the moment, we will consider only errors that may occur on a single qubit, i.e. an erroneous unitary operation.</p>
<p>Any <span class="math inline">\(2^n \times 2^n\)</span> unitary matrix can infact be expressed as a sum of (tensor products of) Pauli operators, with appropriate coefficients, so we consider the general single-qubit error operator</p>
<p><span class="math display">\[ \begin{aligned}
    E = \epsilon_1 I + \epsilon_2 X + \epsilon_3 Z + \epsilon_4 Y.
\end{aligned} \]</span></p>
<p>(Recall that <span class="math inline">\(Y = \text{i}XZ\)</span>.)</p>
<p>Of course, we can correct any of these errors by applying <span class="math inline">\(E^\dagger\)</span>, but in so doing we would in principle need to know the coefficients. It turns out there is a better way.</p>
<p>The first step is to think completely differently about how we would encode a logical qubit.</p>
<p>Consider the operators <span class="math inline">\(X_1, X_2\)</span> and <span class="math inline">\(Z_1, Z_2\)</span>, acting on two qubits. These operators commute, as <span class="math inline">\(XZ = -XZ\)</span> (and operators acting on different qubits always commute.) The (un-normalised) eigenspace is</p>
<p><span class="math display">\[ \begin{aligned}
    \begin{array}{ccc}
        Z_1 Z_2 &amp; X_1 X_2 &amp; \text{Eigenvectors} \\ \hline
          +1 &amp; +1 &amp; |e_1\rangle = |00\rangle + |11\rangle \\
          +1 &amp; -1 &amp; |e_2\rangle = |00\rangle - |11\rangle \\
          -1 &amp; +1 &amp; |e_3\rangle = |01\rangle + |10\rangle \\
          -1 &amp; -1 &amp; |e_4\rangle = |01\rangle - |10\rangle
    \end{array}
\end{aligned} \]</span></p>
<p>They key is this. Consider an arbitrary two-qubit system expressed in this basis</p>
<p><span class="math display">\[ \begin{aligned}
    |\psi\rangle = \alpha |e_1\rangle + \beta|e_2\rangle + \gamma|e_3\rangle + \delta|e_4\rangle.
\end{aligned} \]</span></p>
<p>If we impose the following constraint on this system,</p>
<p><span class="math display">\[ \begin{aligned}
    X_1 X_2 |\psi\rangle = \lambda_X |\psi\rangle
\end{aligned} \]</span></p>
<p>then we cut out two of the possible basis states that the system could be in. For example, say <span class="math inline">\(\lambda_X = +1\)</span>.</p>
<p>Then this rules out the state <span class="math inline">\(|00\rangle - |11\rangle\)</span> and <span class="math inline">\(|01\rangle - |10\rangle\)</span>, leaving us with:</p>
<p><span class="math display">\[ \begin{aligned}
    |\psi\rangle = \alpha|e_1\rangle + \gamma|e_3\rangle.
\end{aligned} \]</span></p>
<p>Observations:</p>
<ul>
<li><p>In general we rule out half the eigenvectors when specifying a stabilizer constraint.</p></li>
<li><p>Suppose we enact <span class="math inline">\(X_1 X_2\)</span> on this new constrained state; it will remain the same, so we can measure in this basis as often as we like, and we will continue to obtain the same eigenvalue when there are no errors.</p></li>
<li><p>If either qubit undergoes say a <span class="math inline">\(Z\)</span> error, the eigenvalue <span class="math inline">\(\lambda_X\)</span> will change. So we can <em>detect</em> this error. We can also correct it, by applying <span class="math inline">\(Z\)</span> again (if we wished), as <span class="math inline">\(Z^2 = 1\)</span>.</p></li>
<li><p>If either qubit undergoes a <span class="math inline">\(X\)</span> error, this is <em>not detectable</em>. We will still be in the <span class="math inline">\(+1\)</span> eigenstate of <span class="math inline">\(X_1 X_2\)</span>, but the coefficients will have switched.</p></li>
</ul>
<p>This is a toy model.</p>
<p>The surface code is a system by which we can detect <em>any</em> single qubit error, <span class="math inline">\(E\)</span> as before, along with several other kinds.</p>
<h2 id="the-surface-code">The surface code</h2>
<p>We define the surface code on a <span class="math inline">\(k \times k\)</span> lattice. At each lattice site we place a qubit. For example, suppose <span class="math inline">\(k = 3\)</span>, then we have</p>
<div class="figure">
<img src="/images/surface_code_1.jpg" />

</div>
<p>Now, we let there be two types of black cricles. Those associated with <span class="math inline">\(X\)</span> operators, and those associated with <span class="math inline">\(Z\)</span>s. We also label the white qubits, as these will form the <em>quiescent</em> state - the stable state of the surface code, and the state which we will manipulate in order to perform computation,</p>
<div class="figure">
<img src="/images/surface_code_3.jpg" />

</div>
<p>In the <span class="math inline">\(k = 3\)</span> system, the implementation of the surface code will enforce the following constraints (with the white qubits labelled as in the above image in red),</p>
<p><span class="math display">\[ \begin{aligned}
    Z_1 Z_2 Z_3 |\psi\rangle = \lambda^{(Z)}_{1,2,3}|\psi\rangle, \\
    Z_3 Z_4 Z_5 |\psi\rangle = \lambda^{(Z)}_{3,4,5}|\psi\rangle, \\
    X_1 X_3 X_4 |\psi\rangle = \lambda^{(X)}_{1,3,4}|\psi\rangle, \\
    X_3 X_4 X_5 |\psi\rangle = \lambda^{(X)}_{3,4,5}|\psi\rangle.
\end{aligned} \]</span></p>
<!--
In general, in the bulk of a surface code lattice, the following two
constraints are enforced, one for each type of measurement qubit, on the state
of all the data qubits:

$$ \begin{aligned}
    X_a X_b X_c X_d |\psi\rangle = \lambda^X_{a,b,c,d}|\psi\rangle, \\
    Z_a Z_b Z_c Z_d |\psi\rangle = \lambda^Z_{a,b,c,d}|\psi\rangle.
\end{aligned} $$
-->
<p>Any implementation of the surface code enforces these constraint at every time step.</p>
<p>In our lattice here, we have <span class="math inline">\(5\)</span> data qubits, and <span class="math inline">\(4\)</span> measurement qubits. We showed that there are <span class="math inline">\(4\)</span> constraints coming from the <span class="math inline">\(4\)</span> data qubits, and as we claimed earlier each of these halve the state space, so the resulting state space has dimension <span class="math inline">\(2^5/2^4 = 2\)</span>. In other words, it encodes one qubit.</p>
<p>Our goal now is to define logical operations on this qubit. In a much larger surface code (and with some modifications to the way logical qubits are encoded on the lattice), it is possible to define all the logical operations necessary to perform universal quantum computation, but here we’ll only look at <span class="math inline">\(X_L\)</span> and <span class="math inline">\(Z_L\)</span>.</p>
<p>Consider an <span class="math inline">\(X\)</span> on data qubit <span class="math inline">\(4\)</span>. This operation commutes with the <span class="math inline">\(X\)</span> stabilizers, but will be detected by the <span class="math inline">\(Z\)</span> stabilier to the right. Consider then the operation <span class="math inline">\(X_L = X_4 X_5\)</span>. This operation commutes with all the stabilizers, and cannot be written in terms of any of them.</p>
<p>So by performing these operators we have obtained <span class="math inline">\(|\psi_X\rangle = X_L|\psi\rangle\)</span> - we’ve manipulated one of the degrees of freedom.</p>
<p>Similarly, we can define <span class="math inline">\(Z_L = Z_2 Z_5\)</span>. Note that this anticommutes with <span class="math inline">\(X_L\)</span>.</p>
<p>So we’ve exhibited some logical operators.</p>
<h3 id="surface-code-facts-and-final-comments">Surface code facts and final comments</h3>
<ul>
<li><p>Tolerant to a 1% error rate in the physical qubit operations,</p></li>
<li><p>Logical qubits in the code look a bit different; they are defined with respect to “defects” in a much larger lattice.</p></li>
<li><p>The estimate in 2012 was that ~14,500 phsyical qubits would be necessary to build one logical qubit.</p></li>
<li><p>In order to factor a 2000 bit number we would require 1 billion qubits.</p></li>
<li><p>I’ve left out a lot of details regarding how errors are corrected - this is not trivial, and it’s actually interesting.</p></li>
<li><p>I’ve not commented on all the types of errors that the surface code corrects against.</p></li>
</ul>
<h2 id="more-information">More information</h2>
<p>There’s the recent surface code paper:</p>
<ul>
<li><a href="https://scirate.com/arxiv/1208.0928">Surface codes: Towards practical large-scale quantum computation</a>,</li>
</ul>
<p>a fantastic recent comprehensive review:</p>
<ul>
<li><a href="https://scirate.com/arxiv/1504.01444">Quantum Computation with Topological codes: from qubit to topological fault-tolerance</a>,</li>
</ul>
<p>there’s a recent video from <a href="http://web.physics.ucsb.edu/~martinisgroup/">John Martinis</a>:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=HQmFEt6l6Tw">Design of a Superconducting Quantum Computer</a>,</li>
</ul>
<p>there’s even some code to go and calculate your own surface code error rates!</p>
<ul>
<li><a href="https://scirate.com/arxiv/1307.0689">Polyestimate: instantaneous open source surface code analysis</a> (<a href="https://github.com/adamcw/autotune">Source code</a>).</li>
</ul>
]]></summary>
</entry>
<entry>
    <title>Feaured Artist - Pablo Amaringo</title>
    <link href="https://silky.github.io/posts/2014-08-14-featured-artist-pablo-amaringo.html" />
    <id>https://silky.github.io/posts/2014-08-14-featured-artist-pablo-amaringo.html</id>
    <published>2014-08-14T00:00:00Z</published>
    <updated>2014-08-14T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="info">
    Posted on August 14, 2014
    
        by Noon van der Silk
    
</div>

<p>Following the invention of the <a href="/posts/2014-08-10-laptop-art-holder.html">laptop art holder</a> I’ve realised I can now feature a particular artist on my laptop every so often. The first of these is <a href="http://en.wikipedia.org/wiki/Pablo_Amaringo">Pablo Amaringo (wiki)</a>, a Peruvian artist whose work is apparently based on the visions he saw while under the influence of <a href="http://en.wikipedia.org/wiki/Ayahuasca">ayahuasca</a>.</p>
<p>His work is featured in a book - <a href="http://www.ayahuascavisions.com/index.html">The Ayahuasca Visions of Pablo Amaringo</a>.</p>
<p>The particular work that is featured on the laptop is:</p>
<p><img src="http://i.imgur.com/UWpxPeo.jpg" width="700" /></p>
<p>but another favourite is (note: monkey magic is hiding in this one; can you find him?):</p>
<p><img src="http://i.imgur.com/Lz2stkr.jpg" width="700" /></p>
<p>In particular I like just how much detail there is in each painting, and everywhere you look you can find something new, if you stare at it for a few moments. (<a href="https://soundcloud.com/pnkslm/goat-mixtape-mastered">This</a> is a pretty good track to listen to while looking at these paintings, by the way.)</p>
<p>It turns out it’s possible to <a href="http://ayahuasca_visions.imagekind.com/store/">buy prints</a> (though seemingly not of the paintings I’ve linked to, which I found by an image search.)</p>
]]></summary>
</entry>

</feed>
