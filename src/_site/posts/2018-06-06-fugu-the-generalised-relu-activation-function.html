<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Noon van der Silk - <tt>fugu</tt>, the generalised <tt>relu</tt> activation function</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script>
        var host = "silky.github.io";
        if ((host == window.location.host) && (window.location.protocol != "https:")) {
          window.location.protocol = "https";
        }
        </script>

        </head>
    <body>
        <div id="header">
            <div id="logo">
                <h1><a href="../">Noon van der Silk</a></h1>
            </div>
            <div id="navigation">
                <a href="../about.html">About</a>
                <a href="../archive.html">Archive</a>
                <a href="../links.html">Links</a>
                <small><a href="../atom.xml">Atom Feed</a></small>
            </div>
        </div>

        <div id="content">
            <h2><tt>fugu</tt>, the generalised <tt>relu</tt> activation function</h2>

            <div class="info">
    Posted on June  6, 2018
    
        by Noon van der Silk
    
</div>

<div>
<p>Recall the standard <code>ReLU</code> function from neural networks:</p>
</div>
<div>
<p><br /><span class="math display">$$
\texttt{ReLU}(x) = \max(0, x) = \begin{cases}
    x &amp; x &gt; 0 \\
    0 &amp; \text{otherwise}
    \end{cases}
$$</span><br /></p>
</div>
<div>
<p>All well-and-good. But what if I want to apply a function to the lower-half of this function, instead of setting it to <span class="math inline">0</span>? Infact, what if I want to apply a function to the top-half as well! And while we’re at it, why should the inflexion point be <span class="math inline">0</span> always?</p>
</div>
<div>
<p>So, here’s the <code>fugu</code> function:</p>
</div>
<div>
<p><br /><span class="math display">$$
\texttt{fugu}(x, f, g, p) = \begin{cases}
    g(x) &amp; x &gt; p \\
    f(x) &amp; \text{otherwise}
    \end{cases}
$$</span><br /></p>
</div>
<div>
<p>Then, <span class="math inline"><code>ReLU</code>(<em>x</em>)=<code>fugu</code>(<em>x</em>, 0, id, 0)</span>, if you wish.</p>
</div>
<div>
<p>Here’s the <code>fugu</code> function in Python TensorFlow:</p>
</div>
<div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> fugu (x, f, g<span class="op">=</span><span class="kw">lambda</span> x: x, point<span class="op">=</span><span class="dv">0</span>):
    cond   <span class="op">=</span> tf.less(x, point)
    <span class="cf">return</span> tf.where(cond, f(x), g(x))</code></pre></div>
</div>
<div>
<p>There, <code>tf.nn.relu(x) = fugu(x, tf.zeros_like)</code>.</p>
</div>
<div>
<p>What kinds of cool/useful functions can you build with this?</p>
</div>
<div>
<p>Exercise: Can you use the <code>fugu</code> function to build a kind of “stairway-to-relu” function?</p>
</div>
<div>
<center>
</div>
<div>
<img src="../images/stairway-to-relu.png" />
</div>
<div>
</center>
</div>

        </div>
        <div id="footer">
            <small><a href="http://jaspervdj.be/hakyll">Hakyll</a> was involved here (<a href="https://github.com/silky/silky.github.com">source</a>).
                I have another <a href="https://sites.google.com/site/noonsilk">website</a>. ن
            </small> 
        </div>
        <div id="image_footer"> &nbsp; </div>
    </body>
</html>
